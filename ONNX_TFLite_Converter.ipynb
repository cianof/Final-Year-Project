{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/onnx/onnx-tensorflow.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "from torch import nn\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.onnx\n",
    "import torchaudio\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class SincConv(torch.nn.Module):\n",
    "    @staticmethod\n",
    "    def to_mel(hz):\n",
    "        return 2595 * np.log10(1 + hz / 700)\n",
    "\n",
    "    @staticmethod\n",
    "    def to_hz(mel):\n",
    "        return 700 * (10 ** (mel / 2595) - 1)\n",
    "\n",
    "    def __init__(self, out_channels, kernel_size, sample_rate=16000, in_channels=1, stride=1, padding=0, min_low_hz=50, min_band_hz=50):\n",
    "        super(SincConv,self).__init__()\n",
    "\n",
    "        if in_channels != 1:\n",
    "            msg = \"SincConv only support one input channel (here, in_channels = {%i})\" % (in_channels)\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        # Forcing the filters to be odd (i.e, perfectly symmetrics)\n",
    "        if kernel_size%2==0:\n",
    "            self.kernel_size=self.kernel_size+1\n",
    "        # parameters    \n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.sample_rate = sample_rate\n",
    "        self.min_low_hz = min_low_hz\n",
    "        self.min_band_hz = min_band_hz\n",
    "        # initialize filterbanks such that they are equally spaced in Mel scale\n",
    "        low_hz = 30\n",
    "        high_hz = self.sample_rate / 2 - (self.min_low_hz + self.min_band_hz)\n",
    "        mel = np.linspace(self.to_mel(low_hz), self.to_mel(high_hz), self.out_channels + 1)\n",
    "        hz = self.to_hz(mel)\n",
    "        # filter lower frequency (out_channels, 1)\n",
    "        self.low_hz_ = torch.nn.Parameter(torch.Tensor(hz[:-1]).view(-1, 1))\n",
    "        # filter frequency band (out_channels, 1)\n",
    "        self.band_hz_ = torch.nn.Parameter(torch.Tensor(np.diff(hz)).view(-1, 1))\n",
    "        # Hamming window\n",
    "        n_lin=torch.linspace(0, (self.kernel_size/2)-1, steps=int((self.kernel_size/2))) # computing only half of the window\n",
    "        self.window_=0.54-0.46*torch.cos(2*math.pi*n_lin/self.kernel_size);\n",
    "        # (1, kernel_size/2)\n",
    "        n = (self.kernel_size - 1) / 2.0\n",
    "        self.n_ = 2*math.pi*torch.arange(-n, 0).view(1, -1) / self.sample_rate # Due to symmetry, I only need half of the time axes\n",
    "\n",
    "    def forward(self, waveforms):\n",
    "        self.n_ = self.n_.to(waveforms.device)\n",
    "        self.window_ = self.window_.to(waveforms.device)\n",
    "        low = self.min_low_hz  + torch.abs(self.low_hz_)\n",
    "        high = torch.clamp(low + self.min_band_hz + torch.abs(self.band_hz_),self.min_low_hz,self.sample_rate/2)\n",
    "        band=(high-low)[:,0]\n",
    "        \n",
    "        f_times_t_low = torch.matmul(low, self.n_)\n",
    "        f_times_t_high = torch.matmul(high, self.n_)\n",
    "\n",
    "        band_pass_left=((torch.sin(f_times_t_high)-torch.sin(f_times_t_low))/(self.n_/2))*self.window_ # Equivalent of Eq.4 of the reference paper (SPEAKER RECOGNITION FROM RAW WAVEFORM WITH SINCNET). I just have expanded the sinc and simplified the terms. This way I avoid several useless computations. \n",
    "        band_pass_center = 2*band.view(-1,1)\n",
    "        band_pass_right= torch.flip(band_pass_left,dims=[1])\n",
    "        \n",
    "        band_pass=torch.cat([band_pass_left,band_pass_center,band_pass_right],dim=1)\n",
    "        band_pass = band_pass / (2*band[:,None])\n",
    "\n",
    "        self.filters = (band_pass).view(self.out_channels, 1, self.kernel_size)\n",
    "        return torch.nn.functional.conv1d(waveforms, self.filters, stride=self.stride, padding=self.padding, dilation=1, bias=None, groups=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogAbs(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogAbs,self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.log(torch.abs(x) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SincConvBlock(c, k, s, sample_rate=16000):\n",
    "    sinc_conv = SincConv(out_channels=c, kernel_size=k, sample_rate=sample_rate, in_channels=1, stride=s, padding=k//2)\n",
    "    avg_pool = torch.nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "    return torch.nn.Sequential(sinc_conv, LogAbs(), torch.nn.BatchNorm1d(c), avg_pool)\n",
    "\n",
    "def DSConvBlock(c_in, c, k, s):\n",
    "    depth_conv = torch.nn.Conv1d(in_channels=c_in, out_channels=c_in, kernel_size=k, stride=s, padding=k//2, groups=c_in)\n",
    "    point_conv = torch.nn.Conv1d(in_channels=c_in, out_channels=c, kernel_size=1, stride=1)\n",
    "    return torch.nn.Sequential(depth_conv, point_conv, torch.nn.ReLU(), torch.nn.BatchNorm1d(c), torch.nn.AvgPool1d(2), torch.nn.Dropout(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinc_conv_model = torch.nn.Sequential(\n",
    "    SincConvBlock(c=40, k=101, s=8),\n",
    "    DSConvBlock(40,160,25,2), \n",
    "    DSConvBlock(160,160,9,1), \n",
    "    DSConvBlock(160,160,9,1), \n",
    "    DSConvBlock(160,160,9,1), \n",
    "    DSConvBlock(160,160,9,1), \n",
    "    torch.nn.AvgPool1d(15),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(160,35),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model =sinc_conv_model\n",
    "model_path = 'Trained_Models/SincConv_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): SincConv()\n",
       "    (1): LogAbs()\n",
       "    (2): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Conv1d(40, 40, kernel_size=(25,), stride=(2,), padding=(12,), groups=40)\n",
       "    (1): Conv1d(40, 160, kernel_size=(1,), stride=(1,))\n",
       "    (2): ReLU()\n",
       "    (3): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): Conv1d(160, 160, kernel_size=(9,), stride=(1,), padding=(4,), groups=160)\n",
       "    (1): Conv1d(160, 160, kernel_size=(1,), stride=(1,))\n",
       "    (2): ReLU()\n",
       "    (3): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (3): Sequential(\n",
       "    (0): Conv1d(160, 160, kernel_size=(9,), stride=(1,), padding=(4,), groups=160)\n",
       "    (1): Conv1d(160, 160, kernel_size=(1,), stride=(1,))\n",
       "    (2): ReLU()\n",
       "    (3): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (4): Sequential(\n",
       "    (0): Conv1d(160, 160, kernel_size=(9,), stride=(1,), padding=(4,), groups=160)\n",
       "    (1): Conv1d(160, 160, kernel_size=(1,), stride=(1,))\n",
       "    (2): ReLU()\n",
       "    (3): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): Conv1d(160, 160, kernel_size=(9,), stride=(1,), padding=(4,), groups=160)\n",
       "    (1): Conv1d(160, 160, kernel_size=(1,), stride=(1,))\n",
       "    (2): ReLU()\n",
       "    (3): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (6): AvgPool1d(kernel_size=(15,), stride=(15,), padding=(0,))\n",
       "  (7): Flatten(start_dim=1, end_dim=-1)\n",
       "  (8): Linear(in_features=160, out_features=35, bias=True)\n",
       "  (9): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch_model.load_state_dict('SinConv_Model.pt')\n",
    "torch_model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "# set the model to inference mode\n",
    "torch_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, fs = torchaudio.load('test1.wav')\n",
    "# wav_libro, sr = librosa.load('test1.wav',sr=16000)\n",
    "inp=waveform.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 16000])\n"
     ]
    }
   ],
   "source": [
    "print(inp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_out = torch_model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2])\n"
     ]
    }
   ],
   "source": [
    "print(torch_out.argmax(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(torch_model, inp, \"sinc_conv.onnx\", opset_version=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_onnx = onnx.load('sinc_conv.onnx')\n",
    "tf_rep = prepare(model_onnx) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_sinc.pb\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_sinc.pb\\assets\n"
     ]
    }
   ],
   "source": [
    "tf_rep.export_graph(\"model_sinc.pb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting SincConv to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('C:/Users/cferr/OneDrive/Documents/4th Year/FYP/model_sinc.pb') # path to the SavedModel directory\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('model_sinc.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting MFCC to TFLite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a much more straightforward transformation as the MFCC model is natively built using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\cferr\\AppData\\Local\\Temp\\tmp4a914hxl\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1122268"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_model_filename = '14_03_2021__21_28.h5'\n",
    "tflite_filename = 'mfcc_model.tflite'\n",
    "\n",
    "# Convert model to TF Lite model\n",
    "model = models.load_model(keras_model_filename)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "open(tflite_filename, 'wb').write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
