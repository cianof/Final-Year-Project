{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SincConv PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjbUjoFXVtoI",
        "outputId": "5993f923-4dcb-4b27-b968-700e8813854c"
      },
      "source": [
        "!pip3 install torchinfo"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/b1/4b310bd715885636e7174b4b52817202fff0ae3609ca2bfb17f28e33e0a1/torchinfo-0.0.8-py3-none-any.whl\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-0.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6onXDSwrRvpl"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import sys\n",
        "from torch.autograd import Variable\n",
        "import math\n",
        "# from torchsummary import summary\n",
        "from torchinfo import summary\n",
        "\n",
        "import os\n",
        "import soundfile as sf\n",
        "from os import listdir\n",
        "from os.path import isdir, join\n",
        "import pathlib\n",
        "import torch.optim as optim\n",
        "import tensorflow as tf\n",
        "\n",
        "output_folder = pathlib.Path('/content/output')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9iK4x4ho3QH"
      },
      "source": [
        "!mkdir output"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGlsAzT9M3Jg",
        "outputId": "eb94bb18-02c8-4cab-ae95-96f77b8a5c08"
      },
      "source": [
        " print(torch.cuda.device_count())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cmyubl-EODT-",
        "outputId": "78751d1b-fcdf-42ec-af70-ddd442ed4317"
      },
      "source": [
        "print(torch.cuda.get_device_name(0))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGfHbeLIFw8p"
      },
      "source": [
        "**Data Import and Processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftCFLnxezOQK",
        "outputId": "9feedb34-4441-4de4-84be-c6f8c73570e3"
      },
      "source": [
        "data_dir = pathlib.Path('/content/data')\n",
        "if not data_dir.exists():\n",
        "  tf.keras.utils.get_file(\n",
        "      'speech_commands_v0.02.tar.gz',\n",
        "      origin=\"http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\",\n",
        "      extract=True,\n",
        "      cache_dir='.', cache_subdir='data')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\n",
            "2428928000/2428923189 [==============================] - 31s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2OUBaTH0EGp",
        "outputId": "29d6a1af-d009-4356-cbbe-1b3e74ec8d87"
      },
      "source": [
        "keywords = [name for name in listdir(data_dir) if isdir(join(data_dir, name))]\n",
        "#remove bg noise as it not needed and requires extra pre-processing\n",
        "keywords.remove('_background_noise_')\n",
        "print(keywords)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['dog', 'follow', 'two', 'bird', 'marvin', 'bed', 'cat', 'four', 'learn', 'eight', 'off', 'left', 'go', 'forward', 'house', 'one', 'stop', 'no', 'backward', 'visual', 'zero', 'sheila', 'yes', 'up', 'wow', 'seven', 'right', 'six', 'happy', 'down', 'on', 'tree', 'nine', 'three', 'five']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdbsGugXF8o-"
      },
      "source": [
        "word2index = {\n",
        "    # core words\n",
        "    \"backward\": 0,\n",
        "    \"bed\": 1,\n",
        "    \"bird\": 2,\n",
        "    \"cat\": 3,\n",
        "    \"dog\": 4,\n",
        "    \"down\": 5,\n",
        "    \"eight\": 6,\n",
        "    \"five\": 7,\n",
        "    \"follow\": 8,\n",
        "    \"forward\": 9,\n",
        "    \"four\": 10,\n",
        "    \"go\": 11,\n",
        "    \"happy\": 12,\n",
        "    \"house\": 13,\n",
        "    \"learn\": 14,\n",
        "    \"left\": 15,\n",
        "    \"marvin\": 16,\n",
        "    \"nine\": 17,\n",
        "    \"no\": 18,\n",
        "    \"off\": 19,\n",
        "    \"on\":20,\n",
        "    \"one\":21,\n",
        "    \"right\":22,\n",
        "    \"seven\":23,\n",
        "    \"sheila\":24,\n",
        "    \"six\":25,\n",
        "    \"stop\":26,\n",
        "    \"three\":27,\n",
        "    \"tree\":28,\n",
        "    \"two\":29,\n",
        "    \"up\":30,\n",
        "    \"visual\":31,\n",
        "    \"wow\":32,\n",
        "    \"yes\":33,\n",
        "    \"zero\":34\n",
        "}\n",
        "\n",
        "index2word = [word for word in word2index]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rM6KKt5EF-ff"
      },
      "source": [
        "filenames = []\n",
        "y = []\n",
        "for word_class in word2index:\n",
        "#     print(join(raw_data_path, trget))\n",
        "#     filenames.append(listdir(join(raw_data_path, target)))\n",
        "    for files in listdir(join(data_dir, word_class)):\n",
        "        filenames.append(join(word_class, files))\n",
        "        y.append(word2index[word_class]) "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eP6qysV6Gh11"
      },
      "source": [
        "#create a dictionary of the filenames and labels\n",
        "combined_dict = dict(zip(filenames,y))\n",
        "#save that dictionary\n",
        "np.save('files_dict', combined_dict)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32oBSUQJGG23"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_data, validation_data, train_classes, validation_classes = train_test_split(filenames, y,\n",
        "                                                                      test_size=0.2, random_state=42, shuffle=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KnbKl9WFQlK"
      },
      "source": [
        "def getLists(files_in, output_name):\n",
        "  MyFile=open(output_name,'w')\n",
        "\n",
        "  for element in files_in:\n",
        "      MyFile.write(element)\n",
        "      MyFile.write('\\n')\n",
        "  MyFile.close()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ndq57rDHFjWU"
      },
      "source": [
        "getLists(filenames, 'fileslist.scp')\n",
        "getLists(validation_data, 'test.scp')\n",
        "getLists(train_data, 'train.scp')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwY59rKzF3Ha"
      },
      "source": [
        "## Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUtU5GJkWfDj"
      },
      "source": [
        "\n",
        "\n",
        "To Implement sinconv I need to:\n",
        "1. Import all relevant pytorch modules\n",
        "2. Correctly copy over all relevant variables that are used in cfg\n",
        "3. Correctly implement the SincConv layer\n",
        "4. Implement my own set of CNN layers.\n",
        "5. Train and test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLE6QrlkXAE5"
      },
      "source": [
        "I must note that there are many different files and classes in SincNet and I have to adjust them to work properly in this .ipynb environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbSYLWXyXKiS"
      },
      "source": [
        "def flip(x, dim):\n",
        "    xsize = x.size()\n",
        "    dim = x.dim() + dim if dim < 0 else dim\n",
        "    x = x.contiguous()\n",
        "    x = x.view(-1, *xsize[dim:])\n",
        "    x = x.view(x.size(0), x.size(1), -1)[:, getattr(torch.arange(x.size(1)-1, \n",
        "                      -1, -1), ('cpu','cuda')[x.is_cuda])().long(), :]\n",
        "    return x.view(xsize)\n",
        "\n",
        "\n",
        "def sinc(band,t_right):\n",
        "    y_right= torch.sin(2*math.pi*band*t_right)/(2*math.pi*band*t_right)\n",
        "    y_left= flip(y_right,0)\n",
        "\n",
        "    y=torch.cat([y_left,Variable(torch.ones(1)).cuda(),y_right])\n",
        "\n",
        "    return y\n",
        "    \n",
        "\n",
        "class SincConv_fast(nn.Module):\n",
        "    \"\"\"Sinc-based convolution\n",
        "    Parameters\n",
        "    ----------\n",
        "    in_channels : `int`\n",
        "        Number of input channels. Must be 1.\n",
        "    out_channels : `int`\n",
        "        Number of filters.\n",
        "    kernel_size : `int`\n",
        "        Filter length.\n",
        "    sample_rate : `int`, optional\n",
        "        Sample rate. Defaults to 16000.\n",
        "    Usage\n",
        "    -----\n",
        "    See `torch.nn.Conv1d`\n",
        "    Reference\n",
        "    ---------\n",
        "    Mirco Ravanelli, Yoshua Bengio,\n",
        "    \"Speaker Recognition from raw waveform with SincNet\".\n",
        "    https://arxiv.org/abs/1808.00158\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def to_mel(hz):\n",
        "        return 2595 * np.log10(1 + hz / 700)\n",
        "\n",
        "    @staticmethod\n",
        "    def to_hz(mel):\n",
        "        return 700 * (10 ** (mel / 2595) - 1)\n",
        "\n",
        "    def __init__(self, out_channels, kernel_size, sample_rate=16000, in_channels=1,\n",
        "                 stride=1, padding=0, dilation=1, bias=False, groups=1, min_low_hz=50, min_band_hz=50):\n",
        "\n",
        "        super(SincConv_fast,self).__init__()\n",
        "\n",
        "        if in_channels != 1:\n",
        "            #msg = (f'SincConv only support one input channel '\n",
        "            #       f'(here, in_channels = {in_channels:d}).')\n",
        "            msg = \"SincConv only support one input channel (here, in_channels = {%i})\" % (in_channels)\n",
        "            raise ValueError(msg)\n",
        "\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        # Forcing the filters to be odd (i.e, perfectly symmetrics)\n",
        "        if kernel_size%2==0:\n",
        "            self.kernel_size=self.kernel_size+1\n",
        "            \n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.dilation = dilation\n",
        "\n",
        "        if bias:\n",
        "            raise ValueError('SincConv does not support bias.')\n",
        "        if groups > 1:\n",
        "            raise ValueError('SincConv does not support groups.')\n",
        "\n",
        "        self.sample_rate = sample_rate\n",
        "        self.min_low_hz = min_low_hz\n",
        "        self.min_band_hz = min_band_hz\n",
        "\n",
        "        # initialize filterbanks such that they are equally spaced in Mel scale\n",
        "        low_hz = 30\n",
        "        high_hz = self.sample_rate / 2 - (self.min_low_hz + self.min_band_hz)\n",
        "\n",
        "        mel = np.linspace(self.to_mel(low_hz),\n",
        "                          self.to_mel(high_hz),\n",
        "                          self.out_channels + 1)\n",
        "        hz = self.to_hz(mel)\n",
        "        \n",
        "\n",
        "        # filter lower frequency (out_channels, 1)\n",
        "        self.low_hz_ = nn.Parameter(torch.Tensor(hz[:-1]).view(-1, 1))\n",
        "\n",
        "        # filter frequency band (out_channels, 1)\n",
        "        self.band_hz_ = nn.Parameter(torch.Tensor(np.diff(hz)).view(-1, 1))\n",
        "\n",
        "        # Hamming window\n",
        "        #self.window_ = torch.hamming_window(self.kernel_size)\n",
        "        n_lin=torch.linspace(0, (self.kernel_size/2)-1, steps=int((self.kernel_size/2))) # computing only half of the window\n",
        "        self.window_=0.54-0.46*torch.cos(2*math.pi*n_lin/self.kernel_size);\n",
        "\n",
        "\n",
        "        # (1, kernel_size/2)\n",
        "        n = (self.kernel_size - 1) / 2.0\n",
        "        self.n_ = 2*math.pi*torch.arange(-n, 0).view(1, -1) / self.sample_rate # Due to symmetry, I only need half of the time axes\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "    def forward(self, waveforms):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        waveforms : `torch.Tensor` (batch_size, 1, n_samples)\n",
        "            Batch of waveforms.\n",
        "        Returns\n",
        "        -------\n",
        "        features : `torch.Tensor` (batch_size, out_channels, n_samples_out)\n",
        "            Batch of sinc filters activations.\n",
        "        \"\"\"\n",
        "\n",
        "        self.n_ = self.n_.to(waveforms.device)\n",
        "\n",
        "        self.window_ = self.window_.to(waveforms.device)\n",
        "\n",
        "        low = self.min_low_hz  + torch.abs(self.low_hz_)\n",
        "        \n",
        "        high = torch.clamp(low + self.min_band_hz + torch.abs(self.band_hz_),self.min_low_hz,self.sample_rate/2)\n",
        "        band=(high-low)[:,0]\n",
        "        \n",
        "        f_times_t_low = torch.matmul(low, self.n_)\n",
        "        f_times_t_high = torch.matmul(high, self.n_)\n",
        "\n",
        "        band_pass_left=((torch.sin(f_times_t_high)-torch.sin(f_times_t_low))/(self.n_/2))*self.window_ # Equivalent of Eq.4 of the reference paper (SPEAKER RECOGNITION FROM RAW WAVEFORM WITH SINCNET). I just have expanded the sinc and simplified the terms. This way I avoid several useless computations. \n",
        "        band_pass_center = 2*band.view(-1,1)\n",
        "        band_pass_right= torch.flip(band_pass_left,dims=[1])\n",
        "        \n",
        "        \n",
        "        band_pass=torch.cat([band_pass_left,band_pass_center,band_pass_right],dim=1)\n",
        "\n",
        "        \n",
        "        band_pass = band_pass / (2*band[:,None])\n",
        "        \n",
        "\n",
        "        self.filters = (band_pass).view(\n",
        "            self.out_channels, 1, self.kernel_size)\n",
        "\n",
        "        return F.conv1d(waveforms, self.filters, stride=self.stride,\n",
        "                        padding=self.padding, dilation=self.dilation,\n",
        "                         bias=None, groups=1) "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dFJl7uDXjQJ"
      },
      "source": [
        "class LayerNorm(nn.Module):\n",
        "\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm,self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(features))\n",
        "        self.beta = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBGWPSaSWJ84"
      },
      "source": [
        "def act_fun(act_type):\n",
        "\n",
        " if act_type==\"relu\":\n",
        "    return nn.ReLU()\n",
        "            \n",
        " if act_type==\"tanh\":\n",
        "    return nn.Tanh()\n",
        "            \n",
        " if act_type==\"sigmoid\":\n",
        "    return nn.Sigmoid()\n",
        "           \n",
        " if act_type==\"leaky_relu\":\n",
        "    return nn.LeakyReLU(0.2)\n",
        "            \n",
        " if act_type==\"elu\":\n",
        "    return nn.ELU()\n",
        "                     \n",
        " if act_type==\"softmax\":\n",
        "    return nn.LogSoftmax()\n",
        "        \n",
        " if act_type==\"linear\":\n",
        "    return nn.LeakyReLU(1)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgdC-BcxdFoU"
      },
      "source": [
        "class SincNet(nn.Module):\n",
        "    \n",
        "    def __init__(self,options):\n",
        "       super(SincNet,self).__init__()\n",
        "    \n",
        "       self.cnn_N_filt=options['cnn_N_filt'] #80,60,60\n",
        "       self.cnn_len_filt=options['cnn_len_filt'] #251,5,5\n",
        "       self.cnn_max_pool_len=options['cnn_max_pool_len']#3,3,3\n",
        "       \n",
        "       \n",
        "       self.cnn_act=options['cnn_act']#leaky_relu\n",
        "       self.cnn_drop=options['cnn_drop']#0.0\n",
        "       \n",
        "       self.cnn_use_laynorm=options['cnn_use_laynorm']#True\n",
        "       self.cnn_use_batchnorm=options['cnn_use_batchnorm']\n",
        "       self.cnn_use_laynorm_inp=options['cnn_use_laynorm_inp']#True\n",
        "       self.cnn_use_batchnorm_inp=options['cnn_use_batchnorm_inp']\n",
        "       \n",
        "       self.input_dim=int(options['input_dim'])#3200 (wlen)\n",
        "       \n",
        "       self.fs=options['fs'] #16,000\n",
        "       \n",
        "       self.N_cnn_lay=len(options['cnn_N_filt'])#3\n",
        "       self.conv  = nn.ModuleList([])\n",
        "       self.bn  = nn.ModuleList([])\n",
        "       self.ln  = nn.ModuleList([])\n",
        "       self.act = nn.ModuleList([])\n",
        "       self.drop = nn.ModuleList([])\n",
        "       self.linear = nn.ModuleList([])\n",
        "       \n",
        "             \n",
        "       if self.cnn_use_laynorm_inp:\n",
        "           self.ln0=LayerNorm(self.input_dim)\n",
        "           \n",
        "       if self.cnn_use_batchnorm_inp:\n",
        "           self.bn0=nn.BatchNorm1d([self.input_dim],momentum=0.05)\n",
        "           \n",
        "       current_input=self.input_dim \n",
        "       \n",
        "       for i in range(self.N_cnn_lay):#loop len=3\n",
        "         \n",
        "         N_filt=int(self.cnn_N_filt[i])#80,60,60\n",
        "         len_filt=int(self.cnn_len_filt[i])#251,5,5\n",
        "         \n",
        "         # dropout\n",
        "         self.drop.append(nn.Dropout(p=self.cnn_drop[i]))#0.0\n",
        "         \n",
        "         # activation\n",
        "         self.act.append(act_fun(self.cnn_act[i]))#leaky_Relu\n",
        "                    \n",
        "         # layer norm initialization         \n",
        "         self.ln.append(LayerNorm([N_filt,int((current_input-self.cnn_len_filt[i]+1)/self.cnn_max_pool_len[i])]))\n",
        "\n",
        "         self.bn.append(nn.BatchNorm1d(N_filt,int((current_input-self.cnn_len_filt[i]+1)/self.cnn_max_pool_len[i]),momentum=0.05))\n",
        "            \n",
        "\n",
        "         if i==0:\n",
        "          self.conv.append(SincConv_fast(self.cnn_N_filt[0],self.cnn_len_filt[0],self.fs))#on first pass send params to SincConv\n",
        "              \n",
        "         else:\n",
        "          self.conv.append(nn.Conv1d(self.cnn_N_filt[i-1], self.cnn_N_filt[i], self.cnn_len_filt[i]))#this has 2 passes - are a Conv1D \n",
        "         current_input=int((current_input-self.cnn_len_filt[i]+1)/self.cnn_max_pool_len[i])\n",
        "\n",
        "       self.linear.append(nn.Linear(6420, 500))\n",
        "       self.linear.append(nn.Linear(500, 35))\n",
        "       self.drop.append(nn.Dropout(p=0.5))\n",
        "       self.act.append(act_fun(self.cnn_act[3]))\n",
        "      #  self.out_dim=current_input*N_filt\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x): #is x inherited from the above function i.e. is it out_dim?\n",
        "       batch=x.shape[0]\n",
        "       seq_len=x.shape[1]\n",
        "       \n",
        "       if bool(self.cnn_use_laynorm_inp):\n",
        "        x=self.ln0((x))\n",
        "        \n",
        "       if bool(self.cnn_use_batchnorm_inp):\n",
        "        x=self.bn0((x))\n",
        "        \n",
        "       x=x.view(batch,1,seq_len)\n",
        "\n",
        "       \n",
        "       for i in range(self.N_cnn_lay):\n",
        "          \n",
        "         if self.cnn_use_laynorm[i]:\n",
        "          if i==0:\n",
        "           x = self.drop[i](self.act[i](self.ln[i](F.max_pool1d(torch.abs(self.conv[i](x)), self.cnn_max_pool_len[i]))))  \n",
        "          else:\n",
        "           x = self.drop[i](self.act[i](self.ln[i](F.max_pool1d(self.conv[i](x), self.cnn_max_pool_len[i]))))   \n",
        "          \n",
        "         if self.cnn_use_batchnorm[i]:\n",
        "          x = self.drop[i](self.act[i](self.bn[i](F.max_pool1d(self.conv[i](x), self.cnn_max_pool_len[i]))))\n",
        "\n",
        "         if self.cnn_use_batchnorm[i]==False and self.cnn_use_laynorm[i]==False:\n",
        "          x = self.drop[i](self.act[i](F.max_pool1d(self.conv[i](x), self.cnn_max_pool_len[i])))\n",
        "\n",
        "         \n",
        "      #  x=torch.flatten(x)\n",
        "       x = x.view(batch,-1)\n",
        "       x=self.linear[0](x)\n",
        "       x=self.linear[1](x)\n",
        "       x=self.drop[3](x) \n",
        "       x=self.act[3](x)\n",
        "      \n",
        "\n",
        "       return x\n",
        "   "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fj1z1Sv9VKEE"
      },
      "source": [
        "#variables for architectures:\n",
        "wlen=3200\n",
        "wshift = 160\n",
        "fs=16000\n",
        "cnn_N_filt=80,60,60\n",
        "# linear_len=6420,500, 35\n",
        "cnn_len_filt=251,5,5\n",
        "cnn_max_pool_len=3,3,3\n",
        "cnn_use_laynorm_inp=True\n",
        "cnn_use_batchnorm_inp=False\n",
        "cnn_use_laynorm=True,True,True\n",
        "cnn_use_batchnorm=False,False,False\n",
        "cnn_act='leaky_relu','leaky_relu','leaky_relu','softmax'\n",
        "cnn_drop=0.0,0.0,0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dm1OgKRZq1Gw"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2PzGryBVIzq",
        "outputId": "d354712a-0f4d-4155-9789-d6e085e8ca4c"
      },
      "source": [
        "CNN_arch = {'input_dim': wlen,\n",
        "          'fs': fs,\n",
        "          'cnn_N_filt': cnn_N_filt,\n",
        "          'cnn_len_filt': cnn_len_filt,\n",
        "          'cnn_max_pool_len':cnn_max_pool_len,\n",
        "          'cnn_use_laynorm_inp': cnn_use_laynorm_inp,\n",
        "          'cnn_use_batchnorm_inp': cnn_use_batchnorm_inp,\n",
        "          'cnn_use_laynorm':cnn_use_laynorm,\n",
        "          'cnn_use_batchnorm':cnn_use_batchnorm,\n",
        "          'cnn_act': cnn_act,\n",
        "          'cnn_drop':cnn_drop,          \n",
        "          }\n",
        "\n",
        "SincLayer=SincNet(CNN_arch)\n",
        "SincLayer.cuda()\n",
        "print(SincLayer)\n",
        "n = count_parameters(SincLayer)\n",
        "print(\"Number of parameters: %s\" % n)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SincNet(\n",
            "  (conv): ModuleList(\n",
            "    (0): SincConv_fast()\n",
            "    (1): Conv1d(80, 60, kernel_size=(5,), stride=(1,))\n",
            "    (2): Conv1d(60, 60, kernel_size=(5,), stride=(1,))\n",
            "  )\n",
            "  (bn): ModuleList(\n",
            "    (0): BatchNorm1d(80, eps=983, momentum=0.05, affine=True, track_running_stats=True)\n",
            "    (1): BatchNorm1d(60, eps=326, momentum=0.05, affine=True, track_running_stats=True)\n",
            "    (2): BatchNorm1d(60, eps=107, momentum=0.05, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (ln): ModuleList(\n",
            "    (0): LayerNorm()\n",
            "    (1): LayerNorm()\n",
            "    (2): LayerNorm()\n",
            "  )\n",
            "  (act): ModuleList(\n",
            "    (0): LeakyReLU(negative_slope=0.2)\n",
            "    (1): LeakyReLU(negative_slope=0.2)\n",
            "    (2): LeakyReLU(negative_slope=0.2)\n",
            "    (3): LogSoftmax(dim=None)\n",
            "  )\n",
            "  (drop): ModuleList(\n",
            "    (0): Dropout(p=0.0, inplace=False)\n",
            "    (1): Dropout(p=0.0, inplace=False)\n",
            "    (2): Dropout(p=0.0, inplace=False)\n",
            "    (3): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (linear): ModuleList(\n",
            "    (0): Linear(in_features=6420, out_features=500, bias=True)\n",
            "    (1): Linear(in_features=500, out_features=35, bias=True)\n",
            "  )\n",
            "  (ln0): LayerNorm()\n",
            ")\n",
            "Number of parameters: 3486355\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkuvkfFc0eGL"
      },
      "source": [
        "data_folder = data_dir\n",
        "outdim = len(keywords)\n",
        "lab_dict = np.load('files_dict.npy', allow_pickle=True).item()\n",
        "N_batches=800\n",
        "N_epochs=10\n",
        "\n",
        "class_lay=35\n",
        "class_drop=0.0\n",
        "class_use_laynorm_inp=False\n",
        "class_use_batchnorm_inp=False\n",
        "class_use_batchnorm=False\n",
        "class_use_laynorm=False\n",
        "class_act='softmax'\n",
        "\n",
        "\n",
        "lr=0.001\n",
        "batch_size=128\n",
        "Batch_dev=128\n",
        "N_epochs=100\n",
        "N_batches=800\n",
        "N_eval_epoch=8\n",
        "seed=42\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# loss function\n",
        "cost = nn.NLLLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK515Ya60w8l"
      },
      "source": [
        "def ReadList(list_file):\n",
        " f=open(list_file,\"r\")\n",
        " lines=f.readlines()\n",
        " list_sig=[]\n",
        " for x in lines:\n",
        "    list_sig.append(x.rstrip())\n",
        " f.close()\n",
        " return list_sig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKrxfKB80ybX"
      },
      "source": [
        "wav_lst_tr=ReadList('train.scp')\n",
        "snt_tr=len(wav_lst_tr)\n",
        "\n",
        "# test list\n",
        "wav_lst_te=ReadList('test.scp')\n",
        "snt_te=len(wav_lst_te)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzLCO0UFgDt6"
      },
      "source": [
        "def create_batches_rnd(batch_size,data_folder,wav_lst,N_snt,wlen,lab_dict,fact_amp):\n",
        "    \n",
        " # Initialization of the minibatch (batch_size,[0=>x_t,1=>x_t+N,1=>random_samp])\n",
        " sig_batch=np.zeros([batch_size,wlen])\n",
        " lab_batch=np.zeros(batch_size)\n",
        "  \n",
        " snt_id_arr=np.random.randint(N_snt, size=batch_size)\n",
        " \n",
        " rand_amp_arr = np.random.uniform(1.0-fact_amp,1+fact_amp,batch_size)\n",
        "\n",
        " for i in range(batch_size):\n",
        "     \n",
        "  # select a random sentence from the list \n",
        "  #[fs,signal]=scipy.io.wavfile.read(data_folder+wav_lst[snt_id_arr[i]])\n",
        "  #signal=signal.astype(float)/32768\n",
        "\n",
        "  [signal, fs] = sf.read(str(data_folder)+'/'+wav_lst[snt_id_arr[i]])\n",
        "\n",
        "  # print(str(data_folder)+'/'+wav_lst[snt_id_arr[i]])\n",
        "  # print(signal.shape)\n",
        "  # accesing to a random chunk\n",
        "  snt_len=signal.shape[0]\n",
        "  snt_beg=np.random.randint(snt_len-wlen-1) #randint(0, snt_len-2*wlen-1)\n",
        "  snt_end=snt_beg+wlen\n",
        "\n",
        "  channels = len(signal.shape)\n",
        "  if channels == 2:\n",
        "    print('WARNING: stereo to mono: '+data_folder+wav_lst[snt_id_arr[i]])\n",
        "    signal = signal[:,0]\n",
        "  \n",
        "  sig_batch[i,:]=signal[snt_beg:snt_end]*rand_amp_arr[i]\n",
        "  lab_batch[i]=lab_dict[wav_lst[snt_id_arr[i]]]\n",
        "  \n",
        " inp=Variable(torch.from_numpy(sig_batch).float().cuda().contiguous())\n",
        " lab=Variable(torch.from_numpy(lab_batch).float().cuda().contiguous())\n",
        "#  print(lab)\n",
        " return inp,lab  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzgRm1D6lq-h",
        "outputId": "b089ce11-efe7-4349-8781-efb5ba16131e"
      },
      "source": [
        "[inp,lab]=create_batches_rnd(128,data_folder,wav_lst_tr,snt_tr,wlen,lab_dict,0.2)\n",
        "print(wlen)\n",
        "print(inp.shape)\n",
        "summary(SincLayer, input_data=[inp])\n",
        "# pout1 =SincLayer(inp)\n",
        "# print('SINC +Conv OUTPUT TENSOR: ')\n",
        "# print(pout1)\n",
        "# pout1=pout1.reshape(128, 856, -1)\n",
        "# print('SINC +Conv OUT SHAPE: ')\n",
        "# print(pout1.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3200\n",
            "torch.Size([128, 3200])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:105: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "├─LayerNorm: 1-1                         [128, 3200]               6,400\n",
              "├─ModuleList: 1                          []                        --\n",
              "|    └─SincConv_fast: 2-1                [128, 80, 2950]           160\n",
              "├─ModuleList: 1                          []                        --\n",
              "|    └─LayerNorm: 2-2                    [128, 80, 983]            157,280\n",
              "├─ModuleList: 1                          []                        --\n",
              "|    └─LeakyReLU: 2-3                    [128, 80, 983]            --\n",
              "├─ModuleList: 1                          []                        --\n",
              "|    └─Dropout: 2-4                      [128, 80, 983]            --\n",
              "├─ModuleList: 1                          []                        --\n",
              "|    └─Conv1d: 2-5                       [128, 60, 979]            24,060\n",
              "├─ModuleList: 1                          []                        --\n",
              "|    └─LayerNorm: 2-6                    [128, 60, 326]            39,120\n",
              "├─ModuleList: 1                          []                        --\n",
              "|    └─LeakyReLU: 2-7                    [128, 60, 326]            --\n",
              "├─ModuleList: 1                          []                        --\n",
              "|    └─Dropout: 2-8                      [128, 60, 326]            --\n",
              "├─ModuleList: 1                          []                        --\n",
              "|    └─Conv1d: 2-9                       [128, 60, 322]            18,060\n",
              "├─ModuleList: 1                          []                        --\n",
              "|    └─LayerNorm: 2-10                   [128, 60, 107]            12,840\n",
              "├─ModuleList: 1                          []                        --\n",
              "|    └─LeakyReLU: 2-11                   [128, 60, 107]            --\n",
              "├─ModuleList: 1                          []                        --\n",
              "|    └─Dropout: 2-12                     [128, 60, 107]            --\n",
              "├─ModuleList: 1                          []                        --\n",
              "|    └─Linear: 2-13                      [128, 500]                3,210,500\n",
              "|    └─Linear: 2-14                      [128, 35]                 17,535\n",
              "├─ModuleList: 1                          []                        --\n",
              "|    └─Dropout: 2-15                     [128, 35]                 --\n",
              "├─ModuleList: 1                          []                        --\n",
              "|    └─LogSoftmax: 2-16                  [128, 35]                 --\n",
              "==========================================================================================\n",
              "Total params: 3,485,955\n",
              "Trainable params: 3,485,955\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 32.52\n",
              "==========================================================================================\n",
              "Input size (MB): 1.64\n",
              "Forward/backward pass size (MB): 432.55\n",
              "Params size (MB): 13.94\n",
              "Estimated Total Size (MB): 448.14\n",
              "=========================================================================================="
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23uCLrL4lj95",
        "outputId": "46636c09-f209-48c1-96a8-b378fa9c7c13"
      },
      "source": [
        "pout1 =SincLayer(inp)\n",
        "print('SINC +Conv OUTPUT TENSOR: ')\n",
        "print(pout1)\n",
        "# pout1=pout1.reshape(128, 856, -1)\n",
        "print('SINC +Conv OUT SHAPE: ')\n",
        "print(pout1.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SINC +Conv OUTPUT TENSOR: \n",
            "tensor([[-3.6250, -3.8474, -3.2508,  ..., -3.6250, -2.9977, -3.6250],\n",
            "        [-3.9154, -3.6919, -3.5398,  ..., -3.3466, -3.9104, -3.9750],\n",
            "        [-3.5921, -3.5921, -3.5921,  ..., -3.1246, -4.1486, -3.0319],\n",
            "        ...,\n",
            "        [-2.8480, -3.7014, -3.1062,  ..., -4.5957, -3.6975, -4.0401],\n",
            "        [-3.6221, -3.6221, -3.6221,  ..., -3.6221, -3.6221, -3.6221],\n",
            "        [-3.7958, -4.1171, -3.6694,  ..., -3.4443, -2.8479, -3.8363]],\n",
            "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
            "SINC +Conv OUT SHAPE: \n",
            "torch.Size([128, 35])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:105: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjsE0L-AnGAP"
      },
      "source": [
        "output_folder = str(output_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LuP0Kzv2TWR",
        "outputId": "427a3ba9-357b-48b4-e345-8b1b5522d678"
      },
      "source": [
        "optimizer_CNN = optim.RMSprop(SincLayer.parameters(), lr=lr,alpha=0.95, eps=1e-8) \n",
        "# optimizer_DNN1 = optim.RMSprop(ConvLayer.parameters(), lr=lr,alpha=0.95, eps=1e-8) \n",
        "# optimizer_DNN2 = optim.RMSprop(ConvLayer2.parameters(), lr=lr,alpha=0.95, eps=1e-8) \n",
        "\n",
        "for epoch in range(N_epochs):\n",
        "  \n",
        "  test_flag=0\n",
        "  SincLayer.train()\n",
        "  # ConvLayer.train()\n",
        "\n",
        " \n",
        "  loss_sum=0\n",
        "  err_sum=0\n",
        "\n",
        "  for i in range(N_batches):\n",
        "\n",
        "    [inp,lab]=create_batches_rnd(batch_size,data_folder,wav_lst_tr,snt_tr,wlen,lab_dict,0.2)\n",
        "    # print(inp.shape)\n",
        "    pout=SincLayer(inp)\n",
        "    # print(pout.shape)\n",
        "    \n",
        "    pred=torch.max(pout,dim=1)[1]\n",
        "    loss = cost(pout, lab.long())\n",
        "    err = torch.mean((pred!=lab.long()).float())\n",
        "    \n",
        "   \n",
        "    \n",
        "    optimizer_CNN.zero_grad()\n",
        "    # optimizer_DNN1.zero_grad() \n",
        "    # optimizer_DNN2.zero_grad() \n",
        "\n",
        "    loss.backward()\n",
        "    optimizer_CNN.step()\n",
        "    # optimizer_DNN1.step()\n",
        "    # optimizer_DNN2.step()\n",
        "    \n",
        "    loss_sum=loss_sum+loss.detach()\n",
        "    err_sum=err_sum+err.detach()\n",
        " \n",
        "\n",
        "  loss_tot=loss_sum/N_batches\n",
        "  err_tot=err_sum/N_batches\n",
        "  \n",
        " \n",
        "   \n",
        "   \n",
        "# Full Validation  new  \n",
        "  if epoch%N_eval_epoch==0:\n",
        "      \n",
        "   SincLayer.eval()\n",
        "  #  ConvLayer.eval()\n",
        "   \n",
        "   test_flag=1 \n",
        "   loss_sum=0\n",
        "   err_sum=0\n",
        "   err_sum_snt=0\n",
        "   \n",
        "   with torch.no_grad():  \n",
        "    for i in range(snt_te):\n",
        "       \n",
        "     #[fs,signal]=scipy.io.wavfile.read(data_folder+wav_lst_te[i])\n",
        "     #signal=signal.astype(float)/32768\n",
        "\n",
        "     [signal, fs] = sf.read(str(data_folder)+'/'+wav_lst_te[i])\n",
        "\n",
        "     signal=torch.from_numpy(signal).float().cuda().contiguous()\n",
        "     lab_batch=lab_dict[wav_lst_te[i]]\n",
        "     \n",
        "     # split signals into chunks\n",
        "     beg_samp=0\n",
        "     end_samp=wlen\n",
        "     \n",
        "     N_fr=int((signal.shape[0]-wlen)/(wshift))\n",
        "     \n",
        "\n",
        "     sig_arr=torch.zeros([Batch_dev,wlen]).float().cuda().contiguous()\n",
        "     lab= Variable((torch.zeros(N_fr+1)+lab_batch).cuda().contiguous().long())\n",
        "     pout=Variable(torch.zeros(N_fr+1,class_lay).float().cuda().contiguous())\n",
        "     count_fr=0\n",
        "     count_fr_tot=0\n",
        "     while end_samp<signal.shape[0]:\n",
        "         sig_arr[count_fr,:]=signal[beg_samp:end_samp]\n",
        "         beg_samp=beg_samp+wshift\n",
        "         end_samp=beg_samp+wlen\n",
        "         count_fr=count_fr+1\n",
        "         count_fr_tot=count_fr_tot+1\n",
        "         if count_fr==Batch_dev:\n",
        "             inp=Variable(sig_arr)\n",
        "             pout[count_fr_tot-Batch_dev:count_fr_tot,:]=SincLayer(inp)\n",
        "             count_fr=0\n",
        "             sig_arr=torch.zeros([Batch_dev,wlen]).float().cuda().contiguous()\n",
        "   \n",
        "     if count_fr>0:\n",
        "      inp=Variable(sig_arr[0:count_fr])\n",
        "      pout[count_fr_tot-count_fr:count_fr_tot,:]=SincLayer(inp)\n",
        "\n",
        "    \n",
        "     pred=torch.max(pout,dim=1)[1]\n",
        "     loss = cost(pout, lab.long())\n",
        "     err = torch.mean((pred!=lab.long()).float())\n",
        "    \n",
        "     [val,best_class]=torch.max(torch.sum(pout,dim=0),0)\n",
        "     err_sum_snt=err_sum_snt+(best_class!=lab[0]).float()\n",
        "    \n",
        "    \n",
        "     loss_sum=loss_sum+loss.detach()\n",
        "     err_sum=err_sum+err.detach()\n",
        "    \n",
        "    err_tot_dev_snt=err_sum_snt/snt_te\n",
        "    loss_tot_dev=loss_sum/snt_te\n",
        "    err_tot_dev=err_sum/snt_te\n",
        "\n",
        "  \n",
        "   print(\"epoch %i, loss_tr=%f err_tr=%f loss_te=%f err_te=%f err_te_snt=%f\" % (epoch, loss_tot,err_tot,loss_tot_dev,err_tot_dev,err_tot_dev_snt))\n",
        "  \n",
        "   with open(str(output_folder)+\"/res.res\", \"a\") as res_file:\n",
        "    res_file.write(\"epoch %i, loss_tr=%f err_tr=%f loss_te=%f err_te=%f err_te_snt=%f\\n\" % (epoch, loss_tot,err_tot,loss_tot_dev,err_tot_dev,err_tot_dev_snt))   \n",
        "\n",
        "   checkpoint={'SincLayer_par': SincLayer.state_dict()\n",
        "               }\n",
        "   torch.save(checkpoint,output_folder+'/model_raw.pkl')\n",
        "  \n",
        "  else:\n",
        "   print(\"epoch %i, loss_tr=%f err_tr=%f\" % (epoch, loss_tot,err_tot))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:105: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 0, loss_tr=3.250765 err_tr=0.861504 loss_te=2.955486 err_te=0.795545 err_te_snt=0.526316\n",
            "epoch 1, loss_tr=3.190054 err_tr=0.843457\n",
            "epoch 2, loss_tr=3.158640 err_tr=0.836406\n",
            "epoch 3, loss_tr=3.125574 err_tr=0.827119\n",
            "epoch 4, loss_tr=3.102938 err_tr=0.821416\n",
            "epoch 5, loss_tr=3.092814 err_tr=0.818437\n",
            "epoch 6, loss_tr=3.067305 err_tr=0.812676\n",
            "epoch 7, loss_tr=3.057657 err_tr=0.809687\n",
            "epoch 8, loss_tr=3.051307 err_tr=0.808281 loss_te=2.701588 err_te=0.716995 err_te_snt=0.339885\n",
            "epoch 9, loss_tr=3.041698 err_tr=0.805771\n",
            "epoch 10, loss_tr=3.032778 err_tr=0.804482\n",
            "epoch 11, loss_tr=3.030358 err_tr=0.803281\n",
            "epoch 12, loss_tr=3.017832 err_tr=0.800049\n",
            "epoch 13, loss_tr=3.005198 err_tr=0.795693\n",
            "epoch 14, loss_tr=3.006149 err_tr=0.798672\n",
            "epoch 15, loss_tr=3.003389 err_tr=0.796133\n",
            "epoch 16, loss_tr=2.996076 err_tr=0.798145 loss_te=2.598333 err_te=0.687237 err_te_snt=0.272513\n",
            "epoch 17, loss_tr=2.990009 err_tr=0.794844\n",
            "epoch 18, loss_tr=2.988358 err_tr=0.794297\n",
            "epoch 19, loss_tr=2.979622 err_tr=0.792344\n",
            "epoch 20, loss_tr=2.968155 err_tr=0.789951\n",
            "epoch 21, loss_tr=2.975254 err_tr=0.790879\n",
            "epoch 22, loss_tr=2.961847 err_tr=0.787910\n",
            "epoch 23, loss_tr=2.958547 err_tr=0.788477\n",
            "epoch 24, loss_tr=2.965070 err_tr=0.790449 loss_te=2.553361 err_te=0.678492 err_te_snt=0.253520\n",
            "epoch 25, loss_tr=2.956917 err_tr=0.787461\n",
            "epoch 26, loss_tr=2.954498 err_tr=0.787109\n",
            "epoch 27, loss_tr=2.948225 err_tr=0.784346\n",
            "epoch 28, loss_tr=2.939723 err_tr=0.784092\n",
            "epoch 29, loss_tr=2.943446 err_tr=0.783535\n",
            "epoch 30, loss_tr=2.937844 err_tr=0.782861\n",
            "epoch 31, loss_tr=2.937614 err_tr=0.783936\n",
            "epoch 32, loss_tr=2.932634 err_tr=0.781494 loss_te=2.478488 err_te=0.662768 err_te_snt=0.241661\n",
            "epoch 33, loss_tr=2.929334 err_tr=0.780596\n",
            "epoch 34, loss_tr=2.923753 err_tr=0.778066\n",
            "epoch 35, loss_tr=2.923620 err_tr=0.778838\n",
            "epoch 36, loss_tr=2.926297 err_tr=0.779961\n",
            "epoch 37, loss_tr=2.918147 err_tr=0.777197\n",
            "epoch 38, loss_tr=2.916469 err_tr=0.778564\n",
            "epoch 39, loss_tr=2.913097 err_tr=0.778125\n",
            "epoch 40, loss_tr=2.915448 err_tr=0.778477 loss_te=2.484125 err_te=0.660043 err_te_snt=0.230086\n",
            "epoch 41, loss_tr=2.913733 err_tr=0.776904\n",
            "epoch 42, loss_tr=2.910000 err_tr=0.776934\n",
            "epoch 43, loss_tr=2.910752 err_tr=0.778115\n",
            "epoch 44, loss_tr=2.909871 err_tr=0.777227\n",
            "epoch 45, loss_tr=2.902211 err_tr=0.775137\n",
            "epoch 46, loss_tr=2.899760 err_tr=0.774775\n",
            "epoch 47, loss_tr=2.898917 err_tr=0.774746\n",
            "epoch 48, loss_tr=2.887898 err_tr=0.771162 loss_te=2.439284 err_te=0.651256 err_te_snt=0.181329\n",
            "epoch 49, loss_tr=2.894545 err_tr=0.773096\n",
            "epoch 50, loss_tr=2.893680 err_tr=0.773662\n",
            "epoch 51, loss_tr=2.886464 err_tr=0.771729\n",
            "epoch 52, loss_tr=2.888794 err_tr=0.769687\n",
            "epoch 53, loss_tr=2.895088 err_tr=0.772988\n",
            "epoch 54, loss_tr=2.891110 err_tr=0.773711\n",
            "epoch 55, loss_tr=2.887725 err_tr=0.771943\n",
            "epoch 56, loss_tr=2.880101 err_tr=0.770693 loss_te=2.408917 err_te=0.642621 err_te_snt=0.192715\n",
            "epoch 57, loss_tr=2.889711 err_tr=0.773389\n",
            "epoch 58, loss_tr=2.883402 err_tr=0.770449\n",
            "epoch 59, loss_tr=2.883078 err_tr=0.772285\n",
            "epoch 60, loss_tr=2.871446 err_tr=0.768770\n",
            "epoch 61, loss_tr=2.873292 err_tr=0.768877\n",
            "epoch 62, loss_tr=2.874215 err_tr=0.769844\n",
            "epoch 63, loss_tr=2.866642 err_tr=0.767168\n",
            "epoch 64, loss_tr=2.869251 err_tr=0.766904 loss_te=2.403542 err_te=0.641551 err_te_snt=0.201408\n",
            "epoch 65, loss_tr=2.863023 err_tr=0.765977\n",
            "epoch 66, loss_tr=2.863338 err_tr=0.765557\n",
            "epoch 67, loss_tr=2.860960 err_tr=0.766221\n",
            "epoch 68, loss_tr=2.858056 err_tr=0.766533\n",
            "epoch 69, loss_tr=2.865339 err_tr=0.767549\n",
            "epoch 70, loss_tr=2.857316 err_tr=0.766846\n",
            "epoch 71, loss_tr=2.862973 err_tr=0.766250\n",
            "epoch 72, loss_tr=2.863638 err_tr=0.768213 loss_te=2.367575 err_te=0.632941 err_te_snt=0.176132\n",
            "epoch 73, loss_tr=2.856482 err_tr=0.763721\n",
            "epoch 74, loss_tr=2.850955 err_tr=0.764902\n",
            "epoch 75, loss_tr=2.853734 err_tr=0.763838\n",
            "epoch 76, loss_tr=2.854353 err_tr=0.765498\n",
            "epoch 77, loss_tr=2.855773 err_tr=0.765459\n",
            "epoch 78, loss_tr=2.844942 err_tr=0.762881\n",
            "epoch 79, loss_tr=2.850135 err_tr=0.763350\n",
            "epoch 80, loss_tr=2.841045 err_tr=0.761924 loss_te=2.366079 err_te=0.627592 err_te_snt=0.182888\n",
            "epoch 81, loss_tr=2.853211 err_tr=0.765342\n",
            "epoch 82, loss_tr=2.852149 err_tr=0.765674\n",
            "epoch 83, loss_tr=2.843417 err_tr=0.763193\n",
            "epoch 84, loss_tr=2.842220 err_tr=0.762051\n",
            "epoch 85, loss_tr=2.835983 err_tr=0.760117\n",
            "epoch 86, loss_tr=2.834916 err_tr=0.760371\n",
            "epoch 87, loss_tr=2.838585 err_tr=0.761201\n",
            "epoch 88, loss_tr=2.835560 err_tr=0.760049 loss_te=2.344828 err_te=0.629961 err_te_snt=0.188463\n",
            "epoch 89, loss_tr=2.837205 err_tr=0.761240\n",
            "epoch 90, loss_tr=2.835165 err_tr=0.759941\n",
            "epoch 91, loss_tr=2.836550 err_tr=0.763096\n",
            "epoch 92, loss_tr=2.837562 err_tr=0.761934\n",
            "epoch 93, loss_tr=2.828467 err_tr=0.759941\n",
            "epoch 94, loss_tr=2.837934 err_tr=0.761475\n",
            "epoch 95, loss_tr=2.825716 err_tr=0.757764\n",
            "epoch 96, loss_tr=2.834611 err_tr=0.761318 loss_te=2.336549 err_te=0.623970 err_te_snt=0.174194\n",
            "epoch 97, loss_tr=2.827296 err_tr=0.759668\n",
            "epoch 98, loss_tr=2.833387 err_tr=0.761582\n",
            "epoch 99, loss_tr=2.836230 err_tr=0.762598\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8iBkRwT3ltA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b4821da-8f90-409f-f1c9-cb968926f972"
      },
      "source": [
        "model = SincNet(CNN_arch)\n",
        "model.load_state_dict(torch.load(output_folder+'/model_raw.pkl'), strict=False)\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SincNet(\n",
              "  (conv): ModuleList(\n",
              "    (0): SincConv_fast()\n",
              "    (1): Conv1d(80, 60, kernel_size=(5,), stride=(1,))\n",
              "    (2): Conv1d(60, 60, kernel_size=(5,), stride=(1,))\n",
              "  )\n",
              "  (bn): ModuleList(\n",
              "    (0): BatchNorm1d(80, eps=983, momentum=0.05, affine=True, track_running_stats=True)\n",
              "    (1): BatchNorm1d(60, eps=326, momentum=0.05, affine=True, track_running_stats=True)\n",
              "    (2): BatchNorm1d(60, eps=107, momentum=0.05, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (ln): ModuleList(\n",
              "    (0): LayerNorm()\n",
              "    (1): LayerNorm()\n",
              "    (2): LayerNorm()\n",
              "  )\n",
              "  (act): ModuleList(\n",
              "    (0): LeakyReLU(negative_slope=0.2)\n",
              "    (1): LeakyReLU(negative_slope=0.2)\n",
              "    (2): LeakyReLU(negative_slope=0.2)\n",
              "    (3): LogSoftmax(dim=None)\n",
              "  )\n",
              "  (drop): ModuleList(\n",
              "    (0): Dropout(p=0.0, inplace=False)\n",
              "    (1): Dropout(p=0.0, inplace=False)\n",
              "    (2): Dropout(p=0.0, inplace=False)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (linear): ModuleList(\n",
              "    (0): Linear(in_features=6420, out_features=500, bias=True)\n",
              "    (1): Linear(in_features=500, out_features=35, bias=True)\n",
              "  )\n",
              "  (ln0): LayerNorm()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pF8LGicqwEiC",
        "outputId": "0db3703b-dfb2-4cec-8d6e-8ca5bbc47723"
      },
      "source": [
        "[inp,lab]=create_batches_rnd(1,data_folder,wav_lst_tr,snt_tr,wlen,lab_dict,0.2)\n",
        "lab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([15.], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 261
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRWiJ-4exSMA"
      },
      "source": [
        "inp=inp.cuda()\n",
        "model = model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NACwp89dxVI4",
        "outputId": "18df39c3-18e7-4922-85b5-304b34c1e06a"
      },
      "source": [
        "prediction = model(inp)\n",
        "pred = torch.max(prediction,dim=1)[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:105: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4271tNhxrHX",
        "outputId": "fd8737f5-1f15-4440-cf44-cad6b1b802c2"
      },
      "source": [
        "pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 264
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--p6YKzAx5FA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}